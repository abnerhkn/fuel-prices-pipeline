{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "345b58da",
   "metadata": {},
   "source": [
    "1. Extração\n",
    "- Nesta etapa, vamos extrair o CSV para a camada mais baixa (bronze) onde os dados brutos serão armazenados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0211d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from datetime import date, timedelta, datetime\n",
    "from glob import glob\n",
    "from zipfile import BadZipFile\n",
    "import unicodedata\n",
    "import locale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddd58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s):\n",
    "\n",
    "    if isinstance(s, str):\n",
    "        return (unicodedata.normalize(\"NFKD\", s)   # separa acentos\n",
    "                .encode(\"ASCII\", \"ignore\")        # remove acentos\n",
    "                .decode(\"utf-8\")                  # volta p/ string\n",
    "                .strip()                          # remove espaços extras\n",
    "                .upper())                         # tudo maiúsculo\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c31814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "def generate_weekly_files():\n",
    "    today = date.today()\n",
    "\n",
    "    \n",
    "    last_week_end = today - timedelta(days=today.weekday() + 1)\n",
    "    \n",
    "    \n",
    "    start = date(2025, 1, 5)\n",
    "    end = start + timedelta(days=6)\n",
    "\n",
    "    urls = []\n",
    "\n",
    "    while end <= last_week_end:\n",
    "        url = (\n",
    "            f\"https://www.gov.br/anp/pt-br/assuntos/precos-e-defesa-da-concorrencia/\"\n",
    "            f\"precos/arquivos-lpc/{end.year}/\"\n",
    "            f\"resumo_semanal_lpc_{start:%Y-%m-%d}_{end:%Y-%m-%d}.xlsx\"\n",
    "        )\n",
    "        urls.append((url, start.strftime(\"%Y-%m-%d\"), end.strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "        \n",
    "        start += timedelta(days=7)\n",
    "        end += timedelta(days=7)\n",
    "\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b980e494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_raw_data():\n",
    "    current_year = date.today().year\n",
    "    base_path = f\"../data/bronze/{current_year}\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    \n",
    "    for urls, week_start, week_end in generate_weekly_files():\n",
    "        date_end = datetime.strptime(week_end, \"%Y-%m-%d\").date()\n",
    "        month = date_end.month\n",
    "\n",
    "        \n",
    "        mkdir_month = f\"{base_path}/{month:02d}\"\n",
    "        os.makedirs(mkdir_month, exist_ok=True)\n",
    "\n",
    "        \n",
    "        file_path = f\"{mkdir_month}/{week_start}_{week_end}.xlsx\"\n",
    "\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            \n",
    "            continue\n",
    "\n",
    "        \n",
    "        resp = requests.get(urls)\n",
    "        if resp.status_code == 200:\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(resp.content)\n",
    "            print(f\"Arquivo salvo: {file_path}\")\n",
    "        else:\n",
    "            print(f\"Erro ao baixar {urls} -> status {resp.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434b0fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558b305b",
   "metadata": {},
   "source": [
    "2. Transformação \n",
    "- Transformando os dados em CSV (dados consumíveis e consistentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aacb9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHEETS = [\"CAPITAIS\", \"MUNICIPIOS\", \"ESTADOS\", \"REGIOES\", \"BRASIL\"]\n",
    "\n",
    "def prepare_silver_structure():\n",
    "    current_year = date.today().year\n",
    "    silver_path = f\"../data/silver/{current_year}/raw\"\n",
    "    os.makedirs(silver_path, exist_ok=True)\n",
    "\n",
    "    \n",
    "    bronze_files = glob(f\"../data/bronze/{current_year}/*/*.xlsx\")\n",
    "    months = {os.path.basename(os.path.dirname(f)) for f in bronze_files}\n",
    "\n",
    "    for month in months:\n",
    "        for sheet in SHEETS:\n",
    "            path = os.path.join(silver_path, month, sheet)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    print(f\"Estrutura Silver criada para {len(months)} meses e {len(SHEETS)} sheets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_silver_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2610c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bronze_to_silver():\n",
    "    current_year = date.today().year\n",
    "    bronze_files = glob(f\"../data/bronze/{current_year}/*/*.xlsx\")\n",
    "    silver_path = f\"../data/silver/{current_year}/raw\"\n",
    "\n",
    "    for file in bronze_files:\n",
    "        try:\n",
    "            month = os.path.basename(os.path.dirname(file))\n",
    "            xls = pd.ExcelFile(file, engine=\"openpyxl\")\n",
    "\n",
    "            for sheet in xls.sheet_names:\n",
    "                try:\n",
    "                    df_temp = pd.read_excel(xls, sheet_name=sheet)\n",
    "                    header_row = df_temp.index[df_temp.iloc[:, 0] == \"DATA INICIAL\"][0]\n",
    "\n",
    "                    df = pd.read_excel(xls, sheet_name=sheet, header=header_row, skiprows=1)\n",
    "\n",
    "                    base_name = os.path.basename(file).replace(\".xlsx\", f\"_{sheet.upper()}.csv\")\n",
    "                    csv_file = os.path.join(silver_path, month, sheet.upper(), base_name)\n",
    "\n",
    "                    if os.path.exists(csv_file):\n",
    "                        continue\n",
    "\n",
    "                    df.to_csv(csv_file, index=False, encoding=\"utf-8-sig\")\n",
    "                    print(f\"Arquivo convertido: {csv_file}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro na sheet {sheet} do arquivo {file}: {e}\")\n",
    "\n",
    "        except (BadZipFile, ValueError) as e:\n",
    "            print(f\"Arquivo inválido (pulado): {file} ({e})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro inesperado em {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae2f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_bronze_to_silver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8958e275",
   "metadata": {},
   "source": [
    "- Verificar o nome de todas as colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bbc76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_unique_silver_columns():\n",
    "    current_year = date.today().year\n",
    "    silver_files = glob(f\"../data/silver/{current_year}/raw/*/*/*.csv\")\n",
    "    \n",
    "    unique_columns = set()\n",
    "\n",
    "    if not silver_files:\n",
    "        print(\"Nenhum arquivo encontrado na camada silver.\")\n",
    "        return set()\n",
    "\n",
    "    for file in silver_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file, nrows=5, encoding=\"utf-8-sig\")\n",
    "            unique_columns.update(df.columns.tolist())\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao abrir {file}: {e}\")\n",
    "\n",
    "    return unique_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadd4c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list_unique_silver_columns()\n",
    "\n",
    "print(\"Colunas únicas encontradas:\")\n",
    "for col in sorted(cols):\n",
    "    print(\"-\", col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fff5b8",
   "metadata": {},
   "source": [
    "- Renomera as colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a7cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_silver_files():\n",
    "\n",
    "    current_year = date.today().year\n",
    "    silver_files = glob(f\"../data/silver/{current_year}/raw/*/*/*.csv\")\n",
    "    \n",
    "\n",
    "    rename_columns = {\n",
    "    \"DATA INICIAL\": \"data_inicial\",\n",
    "    \"DATA FINAL\": \"data_final\",\n",
    "    \"BRASIL\": \"pais\",\n",
    "    \"ESTADO\": \"estado\",\n",
    "    \"ESTADOS\": \"estado\",\n",
    "    \"MUNICÍPIO\": \"municipio\",\n",
    "    \"MUNICIPIO\": \"municipio\",   # <- adicionado\n",
    "    \"REGIAO\": \"regiao\",\n",
    "    \"PRODUTO\": \"produto\",\n",
    "    \"NÚMERO DE POSTOS PESQUISADOS\": \"num_postos_pesquisados\",\n",
    "    \"NUMERO DE POSTOS PESQUISADOS\": \"num_postos_pesquisados\", # <- sem acento\n",
    "    \"UNIDADE DE MEDIDA\": \"unidade_medida\",\n",
    "    \"PREÇO MÉDIO REVENDA\": \"preco_medio_revenda\",\n",
    "    \"PRECO MEDIO REVENDA\": \"preco_medio_revenda\",  # <- sem acento\n",
    "    \"DESVIO PADRÃO REVENDA\": \"desvio_padrao_revenda\",\n",
    "    \"DESVIO PADRAO REVENDA\": \"desvio_padrao_revenda\", # <- sem acento\n",
    "    \"PREÇO MÍNIMO REVENDA\": \"preco_minimo_revenda\",\n",
    "    \"PRECO MINIMO REVENDA\": \"preco_minimo_revenda\",\n",
    "    \"PREÇO MÁXIMO REVENDA\": \"preco_maximo_revenda\",\n",
    "    \"PRECO MAXIMO REVENDA\": \"preco_maximo_revenda\",\n",
    "    \"COEF DE VARIAÇÃO REVENDA\": \"coef_variacao_revenda\",\n",
    "    \"COEF DE VARIACAO REVENDA\": \"coef_variacao_revenda\"\n",
    "}\n",
    "\n",
    "    dfs = []  \n",
    "\n",
    "    for file in silver_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file, encoding=\"utf-8-sig\")\n",
    "\n",
    "            \n",
    "            df.columns = [\n",
    "                unicodedata.normalize(\"NFKD\", col)\n",
    "                .encode(\"ASCII\", \"ignore\")\n",
    "                .decode(\"utf-8\")\n",
    "                .strip()\n",
    "                for col in df.columns\n",
    "            ]\n",
    "\n",
    "            \n",
    "            df = df.rename(columns=rename_columns)\n",
    "\n",
    "            \n",
    "            df.columns = (\n",
    "                df.columns\n",
    "                .str.lower()\n",
    "                .str.strip()\n",
    "                .str.replace(\" \", \"_\")\n",
    "            )\n",
    "\n",
    "            \n",
    "            relative_path = file.replace(\n",
    "                f\"../data/silver/{current_year}/raw\",\n",
    "                f\"../data/silver/{current_year}/raw_normalized/\"\n",
    "            )\n",
    "            os.makedirs(os.path.dirname(relative_path), exist_ok=True)\n",
    "\n",
    "            \n",
    "            df.to_csv(relative_path, index=False, encoding=\"utf-8-sig\")\n",
    "            print(f\"Arquivo padronizado salvo em: {relative_path}\")\n",
    "\n",
    "            \n",
    "            dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar {file}: {e}\")\n",
    "\n",
    "    \n",
    "    if dfs:\n",
    "        return pd.concat(dfs, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d427676",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize_silver_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd7eede",
   "metadata": {},
   "source": [
    "- Modelo Dimensional (Esquema Estrela)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43773a79",
   "metadata": {},
   "source": [
    "  Dimensões:\n",
    "  dim_produto\n",
    "\n",
    "  dim_unidade\n",
    "\n",
    "  dim_regiao\n",
    "\n",
    "  dim_estado\n",
    "\n",
    "  dim_municipio\n",
    "\n",
    "  dim_capitais\n",
    "\n",
    "  dim_pais\n",
    "\n",
    "  dim_tempo\n",
    "  \n",
    "  dim_mes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a172fa65",
   "metadata": {},
   "source": [
    "Fatos:\n",
    "\n",
    "fato_precos_capital\n",
    "\n",
    "fato_precos_municipio\n",
    "\n",
    "fato_precos_estado\n",
    "\n",
    "fato_precos_regiao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507332e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_silver_data():\n",
    "    current_year = date.today().year\n",
    "    silver_files = glob(f\"../data/silver/{current_year}/raw_normalized/*/*/*.csv\")\n",
    "\n",
    "    dfs = []\n",
    "    for file in silver_files:\n",
    "        try:\n",
    "            df_temp = pd.read_csv(\n",
    "                file,\n",
    "                encoding=\"utf-8-sig\",\n",
    "                na_values=[\"NaN\", \"nan\", \"NAN\", \"\"]\n",
    "            )\n",
    "            dfs.append(df_temp)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar {file}: {e}\")\n",
    "\n",
    "    if dfs:\n",
    "        df_silver = pd.concat(dfs, ignore_index=True)\n",
    "        return df_silver\n",
    "    else:\n",
    "        print(\"Nenhum arquivo encontrado na camada Silver.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "df_silver = load_silver_data()\n",
    "\n",
    "print(\"Shape final:\", df_silver.shape)\n",
    "print(\"Colunas:\", df_silver.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99102f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dim_produto(df):\n",
    "    produtos = (\n",
    "        df[\"produto\"]\n",
    "        .dropna()\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    dim_produto = pd.DataFrame({\n",
    "        \"produto_id\": range(1, len(produtos) + 1),\n",
    "        \"produto_descricao\": produtos\n",
    "    })\n",
    "\n",
    "    return dim_produto\n",
    "\n",
    "\n",
    "\n",
    "dim_produto = create_dim_produto(df_silver)\n",
    "\n",
    "\n",
    "print(dim_produto.head(10))\n",
    "print(f\"Total de produtos distintos: {len(dim_produto)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c5e4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dim_unidade(df):\n",
    "\n",
    "    unidades = (\n",
    "        df[\"unidade_medida\"]\n",
    "        .dropna()\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    dim_unidade = pd.DataFrame({\n",
    "        \"unidade_id\": range(1, len(unidades) + 1),\n",
    "        \"unidade_descricao\": unidades\n",
    "    })\n",
    "\n",
    "    return dim_unidade\n",
    "\n",
    "\n",
    "\n",
    "dim_unidade = create_dim_unidade(df_silver)\n",
    "\n",
    "\n",
    "print(dim_unidade.head())\n",
    "print(f\"Total de unidades distintas: {len(dim_unidade)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbee0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dim_tempo(df):\n",
    "    datas = df[[\"data_inicial\", \"data_final\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    datas[\"tempo_id\"] = range(1, len(datas) + 1)\n",
    "    datas[\"ano\"] = pd.to_datetime(datas[\"data_inicial\"]).dt.year\n",
    "    datas[\"mes_id\"] = pd.to_datetime(datas[\"data_inicial\"]).dt.month\n",
    "    datas[\"semana\"] = pd.to_datetime(datas[\"data_inicial\"]).dt.isocalendar().week\n",
    "    datas[\"dia\"] = pd.to_datetime(datas[\"data_inicial\"]).dt.day\n",
    "\n",
    "    return datas[[\"tempo_id\",\"data_inicial\",\"data_final\",\"ano\",\"mes_id\",\"semana\",\"dia\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa3364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dim_mes():\n",
    "    meses = pd.DataFrame({\n",
    "        \"mes_id\": range(1, 13),\n",
    "        \"mes_descricao\": pd.date_range(\"2025-01-01\", periods=12, freq=\"MS\").strftime(\"%B\")\n",
    "    })\n",
    "    return meses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85329486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dim_pais(df):\n",
    "    pais = df[[\"pais\"]].dropna().drop_duplicates().reset_index(drop=True)\n",
    "    pais[\"pais_id\"] = range(1, len(pais) + 1)\n",
    "    pais = pais.rename(columns={\"pais\":\"pais_descricao\"})\n",
    "    return pais[[\"pais_id\",\"pais_descricao\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc38a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_tempo = create_dim_tempo(df_silver)\n",
    "dim_mes = create_dim_mes()\n",
    "dim_pais = create_dim_pais(df_silver)\n",
    "\n",
    "print(dim_tempo.head())\n",
    "print(dim_mes)\n",
    "print(dim_pais)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6ec967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dim_regiao(df):\n",
    "\n",
    "    regiao = (\n",
    "        df[\"regiao\"]\n",
    "        .replace([\"NaN\", \"nan\", \"NAN\", \"\"], pd.NA)  \n",
    "        .dropna()\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    dim_regiao = pd.DataFrame({\n",
    "        \"regiao_id\": range(1, len(regiao) + 1),\n",
    "        \"regiao_descricao\": regiao\n",
    "    })\n",
    "\n",
    "    return dim_regiao\n",
    "\n",
    "\n",
    "\n",
    "dim_regiao = create_dim_regiao(df_silver)\n",
    "\n",
    "\n",
    "print(dim_regiao)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3455864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dim_estado(df, dim_regiao):\n",
    "\n",
    "    estado = (\n",
    "        df[[\"estado\", \"regiao\"]]\n",
    "        .dropna()\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    \n",
    "    estado = estado.merge(\n",
    "        dim_regiao,\n",
    "        left_on=\"regiao\",\n",
    "        right_on=\"regiao_descricao\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    estado[\"estado_id\"] = range(1, len(estado) + 1)\n",
    "\n",
    "    \n",
    "    estado = estado.rename(columns={\"estado\": \"estado_descricao\"})\n",
    "\n",
    "    return estado[[\"estado_id\", \"estado_descricao\", \"regiao_id\"]]\n",
    "\n",
    "\n",
    "\n",
    "dim_estado = create_dim_estado(df_silver, dim_regiao)\n",
    "\n",
    "\n",
    "print(dim_estado.head())\n",
    "print(f\"Total de estados distintos: {len(dim_estado)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d509a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dim_municipio(df, dim_estado):\n",
    "\n",
    "    capitais_list = [\n",
    "        \"RIO BRANCO\",\"MACEIO\",\"MACAPA\",\"MANAUS\",\"SALVADOR\",\"FORTALEZA\",\"BRASILIA\",\n",
    "        \"VITORIA\",\"GOIANIA\",\"SAO LUIS\",\"CUIABA\",\"CAMPO GRANDE\",\"BELO HORIZONTE\",\n",
    "        \"BELEM\",\"JOAO PESSOA\",\"CURITIBA\",\"RECIFE\",\"TERESINA\",\"RIO DE JANEIRO\",\n",
    "        \"NATAL\",\"PORTO ALEGRE\",\"PORTO VELHO\",\"BOA VISTA\",\"FLORIANOPOLIS\",\n",
    "        \"SAO PAULO\",\"ARACAJU\",\"PALMAS\"\n",
    "    ]\n",
    "\n",
    "    \n",
    "    municipio = (\n",
    "        df.loc[df[\"municipio\"].notna(), [\"municipio\",\"estado\"]]\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    \n",
    "    municipio = municipio.merge(\n",
    "        dim_estado[[\"estado_id\",\"estado_descricao\",\"regiao_id\"]],\n",
    "        left_on=\"estado\", right_on=\"estado_descricao\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    municipio[\"municipio_id\"] = range(1, len(municipio)+1)\n",
    "    municipio[\"is_capital\"] = municipio[\"municipio\"].isin(capitais_list).astype(int)\n",
    "\n",
    "    \n",
    "    dim_municipio = municipio.rename(columns={\"municipio\":\"municipio_descricao\"})\n",
    "    dim_municipio = dim_municipio[[\"municipio_id\",\"municipio_descricao\",\"regiao_id\",\"estado_id\",\"is_capital\"]]\n",
    "\n",
    "    return dim_municipio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913af3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_municipio = create_dim_municipio(df_silver, dim_estado)\n",
    "\n",
    "print(dim_municipio.head(10))\n",
    "print(f\"Total municípios distintos: {len(dim_municipio)}\")\n",
    "print(f\"Total capitais detectadas: {dim_municipio['is_capital'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6fdc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dimensions(\n",
    "    dim_produto, dim_unidade, dim_regiao, dim_estado,\n",
    "    dim_municipio, dim_tempo, dim_mes, dim_pais\n",
    "):\n",
    "    current_year = date.today().year\n",
    "    base_path = f\"../data/gold/{current_year}/dim\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    dim_produto.to_csv(f\"{base_path}/dim_produto.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    dim_unidade.to_csv(f\"{base_path}/dim_unidade.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    dim_regiao.to_csv(f\"{base_path}/dim_regiao.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    dim_estado.to_csv(f\"{base_path}/dim_estado.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    dim_municipio.to_csv(f\"{base_path}/dim_municipio.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    dim_tempo.to_csv(f\"{base_path}/dim_tempo.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    dim_mes.to_csv(f\"{base_path}/dim_mes.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    dim_pais.to_csv(f\"{base_path}/dim_pais.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"Todas as dimensões foram salvas em: {base_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dimensions(\n",
    "    dim_produto, dim_unidade, dim_regiao, dim_estado,\n",
    "    dim_municipio, dim_tempo, dim_mes, dim_pais\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_year = date.today().year\n",
    "dim_path = f\"../data/gold/{current_year}/dim\"\n",
    "\n",
    "dim_produto    = pd.read_csv(f\"{dim_path}/dim_produto.csv\", encoding=\"utf-8-sig\")\n",
    "dim_unidade    = pd.read_csv(f\"{dim_path}/dim_unidade.csv\", encoding=\"utf-8-sig\")\n",
    "dim_regiao     = pd.read_csv(f\"{dim_path}/dim_regiao.csv\", encoding=\"utf-8-sig\")\n",
    "dim_estado     = pd.read_csv(f\"{dim_path}/dim_estado.csv\", encoding=\"utf-8-sig\")\n",
    "dim_municipio  = pd.read_csv(f\"{dim_path}/dim_municipio.csv\", encoding=\"utf-8-sig\")\n",
    "dim_tempo      = pd.read_csv(f\"{dim_path}/dim_tempo.csv\", encoding=\"utf-8-sig\")\n",
    "dim_mes        = pd.read_csv(f\"{dim_path}/dim_mes.csv\", encoding=\"utf-8-sig\")\n",
    "dim_pais       = pd.read_csv(f\"{dim_path}/dim_pais.csv\", encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c202073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_files = glob(f\"../data/silver/{current_year}/raw_normalized/*/*/*.csv\")\n",
    "df_list = [pd.read_csv(file, encoding=\"utf-8-sig\") for file in silver_files]\n",
    "df_silver = pd.concat(df_list, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea8a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df, name, key in [\n",
    "    (dim_produto, \"dim_produto\", \"produto_id\"),\n",
    "    (dim_unidade, \"dim_unidade\", \"unidade_id\"),\n",
    "    (dim_regiao, \"dim_regiao\", \"regiao_id\"),\n",
    "    (dim_estado, \"dim_estado\", \"estado_id\"),\n",
    "    (dim_municipio, \"dim_municipio\", \"municipio_id\"),\n",
    "    (dim_tempo, \"dim_tempo\", \"tempo_id\"),\n",
    "    (dim_mes, \"dim_mes\", \"mes_id\"),\n",
    "    (dim_pais, \"dim_pais\", \"pais_id\"),\n",
    "]:\n",
    "    print(f\"{name}: total={len(df)}, ids únicos={df[key].nunique()}, nulos={df[key].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e40a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Estados sem região:\", dim_estado[~dim_estado[\"regiao_id\"].isin(dim_regiao[\"regiao_id\"])])\n",
    "print(\"Municípios sem estado:\", dim_municipio[~dim_municipio[\"estado_id\"].isin(dim_estado[\"estado_id\"])])\n",
    "print(\"Municípios sem região:\", dim_municipio[~dim_municipio[\"regiao_id\"].isin(dim_regiao[\"regiao_id\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf9d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"produto_descricao\",\"unidade_descricao\",\"regiao_descricao\",\n",
    "            \"estado_descricao\",\"municipio_descricao\",\"mes_descricao\",\"pais_descricao\"]:\n",
    "    for df, name in [(dim_produto,\"dim_produto\"), (dim_unidade,\"dim_unidade\"),\n",
    "                     (dim_regiao,\"dim_regiao\"), (dim_estado,\"dim_estado\"),\n",
    "                     (dim_municipio,\"dim_municipio\"), (dim_mes,\"dim_mes\"), (dim_pais,\"dim_pais\")]:\n",
    "        if col in df.columns:\n",
    "            print(f\"{name} - {col}: nulos = {df[col].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfe6ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dim_estado colunas:\", dim_estado.columns.tolist())\n",
    "print(\"dim_municipio colunas:\", dim_municipio.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "41798271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fato_precos_municipio(df, dim_produto, dim_unidade, dim_tempo,\n",
    "                                 dim_pais, dim_estado, dim_municipio):\n",
    "    \n",
    "    fato = df.merge(\n",
    "        dim_produto[[\"produto_id\", \"produto_descricao\"]],\n",
    "        left_on=\"produto\", right_on=\"produto_descricao\", how=\"left\"\n",
    "    )\n",
    "    print(\"Após merge produto:\", fato.columns.tolist())\n",
    "\n",
    "    \n",
    "    fato = fato.merge(\n",
    "        dim_unidade[[\"unidade_id\", \"unidade_descricao\"]],\n",
    "        left_on=\"unidade_medida\", right_on=\"unidade_descricao\", how=\"left\"\n",
    "    )\n",
    "    print(\"Após merge unidade:\", fato.columns.tolist())\n",
    "\n",
    "    \n",
    "    fato = fato.merge(\n",
    "        dim_tempo[[\"tempo_id\",\"data_inicial\",\"data_final\",\"ano\",\"mes_id\",\"semana\"]],\n",
    "        on=[\"data_inicial\",\"data_final\"], how=\"left\"\n",
    "    )\n",
    "    print(\"Após merge tempo:\", fato.columns.tolist())\n",
    "\n",
    "    \n",
    "    fato[\"pais_id\"] = 1\n",
    "\n",
    "    \n",
    "    fato = fato.merge(\n",
    "        dim_estado[[\"estado_id\",\"estado_norm\",\"regiao_id\"]],\n",
    "        left_on=\"estado\", right_on=\"estado_norm\", how=\"left\"\n",
    "    )\n",
    "    print(\"Após merge estado:\", fato.columns.tolist())\n",
    "\n",
    "    \n",
    "    fato = fato.merge(\n",
    "        dim_municipio[[\"municipio_id\",\"municipio_norm\",\"estado_id\",\"regiao_id\",\"is_capital\"]],\n",
    "        left_on=\"municipio\", right_on=\"municipio_norm\", how=\"left\",\n",
    "        suffixes=(\"_estado\",\"_municipio\")\n",
    "    )\n",
    "    print(\"Após merge municipio:\", fato.columns.tolist())\n",
    "\n",
    "    \n",
    "    fato = fato.rename(columns={\n",
    "        \"estado_id_municipio\": \"estado_id\",\n",
    "        \"regiao_id_municipio\": \"regiao_id\"\n",
    "    })\n",
    "\n",
    "    fato = fato[[\n",
    "        \"data_inicial\",\"data_final\",\"ano\",\"mes_id\",\"semana\",\n",
    "        \"produto_id\",\"unidade_id\",\"pais_id\",\"regiao_id\",\"estado_id\",\"municipio_id\",\n",
    "        \"is_capital\",\"num_postos_pesquisados\",\n",
    "        \"preco_medio_revenda\",\"preco_minimo_revenda\",\"preco_maximo_revenda\",\n",
    "        \"desvio_padrao_revenda\",\"coef_variacao_revenda\"\n",
    "    ]]\n",
    "\n",
    "    \n",
    "    fato = fato.dropna(subset=[\"estado_id\",\"municipio_id\",\"regiao_id\"]).reset_index(drop=True)\n",
    "    fato = fato.astype({\n",
    "        \"estado_id\": \"int64\",\n",
    "        \"municipio_id\": \"int64\",\n",
    "        \"regiao_id\": \"int64\",\n",
    "        \"is_capital\": \"int64\"\n",
    "    })\n",
    "\n",
    "    return fato\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "34db5011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após merge produto: ['data_inicial', 'data_final', 'pais', 'produto', 'num_postos_pesquisados', 'unidade_medida', 'preco_medio_revenda', 'desvio_padrao_revenda', 'preco_minimo_revenda', 'preco_maximo_revenda', 'coef_variacao_revenda', 'estado', 'municipio', 'regiao', 'produto_id', 'produto_descricao']\n",
      "Após merge unidade: ['data_inicial', 'data_final', 'pais', 'produto', 'num_postos_pesquisados', 'unidade_medida', 'preco_medio_revenda', 'desvio_padrao_revenda', 'preco_minimo_revenda', 'preco_maximo_revenda', 'coef_variacao_revenda', 'estado', 'municipio', 'regiao', 'produto_id', 'produto_descricao', 'unidade_id', 'unidade_descricao']\n",
      "Após merge tempo: ['data_inicial', 'data_final', 'pais', 'produto', 'num_postos_pesquisados', 'unidade_medida', 'preco_medio_revenda', 'desvio_padrao_revenda', 'preco_minimo_revenda', 'preco_maximo_revenda', 'coef_variacao_revenda', 'estado', 'municipio', 'regiao', 'produto_id', 'produto_descricao', 'unidade_id', 'unidade_descricao', 'tempo_id', 'ano', 'mes_id', 'semana']\n",
      "Após merge estado: ['data_inicial', 'data_final', 'pais', 'produto', 'num_postos_pesquisados', 'unidade_medida', 'preco_medio_revenda', 'desvio_padrao_revenda', 'preco_minimo_revenda', 'preco_maximo_revenda', 'coef_variacao_revenda', 'estado', 'municipio', 'regiao', 'produto_id', 'produto_descricao', 'unidade_id', 'unidade_descricao', 'tempo_id', 'ano', 'mes_id', 'semana', 'pais_id', 'estado_id', 'estado_norm', 'regiao_id']\n",
      "Após merge municipio: ['data_inicial', 'data_final', 'pais', 'produto', 'num_postos_pesquisados', 'unidade_medida', 'preco_medio_revenda', 'desvio_padrao_revenda', 'preco_minimo_revenda', 'preco_maximo_revenda', 'coef_variacao_revenda', 'estado', 'municipio', 'regiao', 'produto_id', 'produto_descricao', 'unidade_id', 'unidade_descricao', 'tempo_id', 'ano', 'mes_id', 'semana', 'pais_id', 'estado_id_estado', 'estado_norm', 'regiao_id_estado', 'municipio_id', 'municipio_norm', 'estado_id_municipio', 'regiao_id_municipio', 'is_capital']\n",
      "\n",
      "Fato Municipio pronto\n",
      "  data_inicial  data_final   ano  mes_id  semana  produto_id  unidade_id  \\\n",
      "0   2025-01-05  2025-01-11  2025       1       1           1           1   \n",
      "1   2025-01-05  2025-01-11  2025       1       1           1           1   \n",
      "2   2025-01-05  2025-01-11  2025       1       1           1           1   \n",
      "3   2025-01-05  2025-01-11  2025       1       1           1           1   \n",
      "4   2025-01-05  2025-01-11  2025       1       1           1           1   \n",
      "\n",
      "   pais_id  regiao_id  estado_id  municipio_id  is_capital  \\\n",
      "0        1          2         10             1           0   \n",
      "1        1          3          9             2           0   \n",
      "2        1          2          5             3           0   \n",
      "3        1          4         13             4           0   \n",
      "4        1          1         14             5           0   \n",
      "\n",
      "   num_postos_pesquisados  preco_medio_revenda  preco_minimo_revenda  \\\n",
      "0                       5                 4.66                  4.39   \n",
      "1                       8                 4.01                  3.98   \n",
      "2                       8                 4.24                  4.13   \n",
      "3                       8                 4.05                  3.99   \n",
      "4                       2                 5.48                  5.48   \n",
      "\n",
      "   preco_maximo_revenda  desvio_padrao_revenda  coef_variacao_revenda  \n",
      "0                  4.84               0.204939                  0.044  \n",
      "1                  4.29               0.109602                  0.027  \n",
      "2                  4.49               0.121177                  0.029  \n",
      "3                  4.27               0.113137                  0.028  \n",
      "4                  5.48               0.000000                  0.000  \n",
      "Tipos de dados:\n",
      " data_inicial               object\n",
      "data_final                 object\n",
      "ano                         int64\n",
      "mes_id                      int64\n",
      "semana                      int64\n",
      "produto_id                  int64\n",
      "unidade_id                  int64\n",
      "pais_id                     int64\n",
      "regiao_id                   int64\n",
      "estado_id                   int64\n",
      "municipio_id                int64\n",
      "is_capital                  int64\n",
      "num_postos_pesquisados      int64\n",
      "preco_medio_revenda       float64\n",
      "preco_minimo_revenda      float64\n",
      "preco_maximo_revenda      float64\n",
      "desvio_padrao_revenda     float64\n",
      "coef_variacao_revenda     float64\n",
      "dtype: object\n",
      "Total de linhas: 75185\n"
     ]
    }
   ],
   "source": [
    "fato_municipio = create_fato_precos_municipio(\n",
    "    df_silver,\n",
    "    dim_produto,\n",
    "    dim_unidade,\n",
    "    dim_tempo,\n",
    "    dim_pais,\n",
    "    dim_estado,\n",
    "    dim_municipio\n",
    ")\n",
    "\n",
    "print(\"\\nFato Municipio pronto\")\n",
    "print(fato_municipio.head())\n",
    "print(\"Tipos de dados:\\n\", fato_municipio.dtypes)\n",
    "print(\"Total de linhas:\", len(fato_municipio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "57ea126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fato(df: pd.DataFrame, name: str):\n",
    "    current_year = date.today().year\n",
    "    base_path = f\"../data/gold/{current_year}/fato\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(base_path, f\"{name}.csv\")\n",
    "    df.to_csv(file_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"Fato {name} salvo em {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "2f8a5de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fato fato_precos_municipio salvo em ../data/gold/2025/fato/fato_precos_municipio.csv\n"
     ]
    }
   ],
   "source": [
    "save_fato(fato_municipio, \"fato_precos_municipio\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "a91962fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fato_precos_estado(df, dim_produto, dim_unidade, dim_tempo, dim_estado, dim_regiao):\n",
    "    fato = (\n",
    "        df.merge(dim_produto[[\"produto_id\", \"produto_descricao\"]],\n",
    "                 left_on=\"produto\", right_on=\"produto_descricao\", how=\"left\")\n",
    "          .merge(dim_unidade[[\"unidade_id\", \"unidade_descricao\"]],\n",
    "                 left_on=\"unidade_medida\", right_on=\"unidade_descricao\", how=\"left\")\n",
    "          .merge(dim_tempo[[\"tempo_id\",\"data_inicial\",\"data_final\",\"ano\",\"mes_id\",\"semana\"]],\n",
    "                 on=[\"data_inicial\",\"data_final\"], how=\"left\")\n",
    "          .merge(dim_estado[[\"estado_id\",\"estado_norm\",\"regiao_id\"]],\n",
    "                 left_on=\"estado\", right_on=\"estado_norm\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    \n",
    "    fato[\"pais_id\"] = 1\n",
    "\n",
    "    \n",
    "    fato_agg = fato.groupby(\n",
    "        [\"data_inicial\",\"data_final\",\"ano\",\"mes_id\",\"semana\",\n",
    "        \"produto_id\",\"unidade_id\",\"pais_id\",\"regiao_id\",\"estado_id\"],\n",
    "        as_index=False\n",
    "    ).agg({\n",
    "        \"num_postos_pesquisados\": \"sum\",\n",
    "        \"preco_medio_revenda\": \"mean\",\n",
    "        \"preco_minimo_revenda\": \"min\",\n",
    "        \"preco_maximo_revenda\": \"max\",\n",
    "        \"desvio_padrao_revenda\": \"mean\",\n",
    "        \"coef_variacao_revenda\": \"mean\"\n",
    "    })\n",
    "\n",
    "    fato_agg = fato_agg.astype({\n",
    "        \"estado_id\": \"int64\",\n",
    "        \"regiao_id\": \"int64\",\n",
    "        \"pais_id\": \"int64\"\n",
    "    })\n",
    "\n",
    "    return fato_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "dadf856b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  data_inicial  data_final   ano  mes_id  semana  produto_id  unidade_id  \\\n",
      "0   2025-01-05  2025-01-11  2025       1       1           1           1   \n",
      "1   2025-01-05  2025-01-11  2025       1       1           1           1   \n",
      "2   2025-01-05  2025-01-11  2025       1       1           1           1   \n",
      "3   2025-01-05  2025-01-11  2025       1       1           1           1   \n",
      "4   2025-01-05  2025-01-11  2025       1       1           1           1   \n",
      "\n",
      "   pais_id  regiao_id  estado_id  num_postos_pesquisados  preco_medio_revenda  \\\n",
      "0        1          1          1                       9              5.23250   \n",
      "1        1          1          3                       6              5.11000   \n",
      "2        1          1          4                     101              5.00250   \n",
      "3        1          1         14                      75              4.85125   \n",
      "4        1          1         22                      35              5.10200   \n",
      "\n",
      "   preco_minimo_revenda  preco_maximo_revenda  desvio_padrao_revenda  \\\n",
      "0                  5.09                  5.74               0.092651   \n",
      "1                  5.11                  5.11               0.000000   \n",
      "2                  4.97                  5.09               0.032189   \n",
      "3                  4.28                  5.48               0.142070   \n",
      "4                  4.79                  5.39               0.161287   \n",
      "\n",
      "   coef_variacao_revenda  \n",
      "0                0.01775  \n",
      "1                0.00000  \n",
      "2                0.00650  \n",
      "3                0.03000  \n",
      "4                0.03180  \n",
      "data_inicial               object\n",
      "data_final                 object\n",
      "ano                         int64\n",
      "mes_id                      int64\n",
      "semana                      int64\n",
      "produto_id                  int64\n",
      "unidade_id                  int64\n",
      "pais_id                     int64\n",
      "regiao_id                   int64\n",
      "estado_id                   int64\n",
      "num_postos_pesquisados      int64\n",
      "preco_medio_revenda       float64\n",
      "preco_minimo_revenda      float64\n",
      "preco_maximo_revenda      float64\n",
      "desvio_padrao_revenda     float64\n",
      "coef_variacao_revenda     float64\n",
      "dtype: object\n",
      "Total de linhas: 5893\n"
     ]
    }
   ],
   "source": [
    "fato_estado = create_fato_precos_estado(\n",
    "    df_silver,\n",
    "    dim_produto,\n",
    "    dim_unidade,\n",
    "    dim_tempo,\n",
    "    dim_estado,\n",
    "    dim_regiao\n",
    ")\n",
    "\n",
    "print(fato_estado.head())\n",
    "print(fato_estado.dtypes)\n",
    "print(\"Total de linhas:\", len(fato_estado))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "b28a53d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fato fato_precos_estado salvo em ../data/gold/2025/fato/fato_precos_estado.csv\n"
     ]
    }
   ],
   "source": [
    "save_fato(fato_estado, \"fato_precos_estado\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "c2f888e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fato_precos_regiao(df, dim_produto, dim_unidade, dim_tempo, dim_regiao):\n",
    "    fato = (\n",
    "        df.merge(dim_produto[[\"produto_id\", \"produto_descricao\"]],\n",
    "                 left_on=\"produto\", right_on=\"produto_descricao\", how=\"left\")\n",
    "          .merge(dim_unidade[[\"unidade_id\", \"unidade_descricao\"]],\n",
    "                 left_on=\"unidade_medida\", right_on=\"unidade_descricao\", how=\"left\")\n",
    "          .merge(dim_tempo[[\"tempo_id\",\"data_inicial\",\"data_final\",\"ano\",\"mes_id\",\"semana\"]],\n",
    "                 on=[\"data_inicial\",\"data_final\"], how=\"left\")\n",
    "          .merge(dim_regiao[[\"regiao_id\",\"regiao_descricao\"]],\n",
    "                 left_on=\"regiao\", right_on=\"regiao_descricao\", how=\"left\")\n",
    "    )\n",
    "\n",
    "    \n",
    "    fato[\"pais_id\"] = 1\n",
    "\n",
    "    \n",
    "    fato_agg = fato.groupby(\n",
    "        [\"data_inicial\",\"data_final\",\"ano\",\"mes_id\",\"semana\",\n",
    "         \"produto_id\",\"unidade_id\",\"pais_id\",\"regiao_id\"],\n",
    "        as_index=False\n",
    "    ).agg({\n",
    "        \"num_postos_pesquisados\": \"sum\",\n",
    "        \"preco_medio_revenda\": \"mean\",\n",
    "        \"preco_minimo_revenda\": \"min\",\n",
    "        \"preco_maximo_revenda\": \"max\",\n",
    "        \"desvio_padrao_revenda\": \"mean\",      \n",
    "        \"coef_variacao_revenda\": \"mean\"       \n",
    "    })\n",
    "\n",
    "    \n",
    "    fato_agg = fato_agg.astype({\n",
    "        \"regiao_id\": \"int64\",\n",
    "        \"pais_id\": \"int64\"\n",
    "    })\n",
    "\n",
    "    return fato_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "e7bb9714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  data_inicial  data_final   ano  mes_id  semana  produto_id  unidade_id  \\\n",
      "0   2025-01-05  2025-01-11  2025       1       1           1           1   \n",
      "1   2025-01-05  2025-01-11  2025       1       1           1           1   \n",
      "2   2025-01-05  2025-01-11  2025       1       1           1           1   \n",
      "3   2025-01-05  2025-01-11  2025       1       1           1           1   \n",
      "4   2025-01-05  2025-01-11  2025       1       1           1           1   \n",
      "\n",
      "   pais_id  regiao_id  num_postos_pesquisados  preco_medio_revenda  \\\n",
      "0        1          1                     260                4.980   \n",
      "1        1          2                    1372                4.457   \n",
      "2        1          3                     632                4.072   \n",
      "3        1          4                    3792                4.214   \n",
      "4        1          5                     928                4.475   \n",
      "\n",
      "   preco_minimo_revenda  preco_maximo_revenda  desvio_padrao_revenda  \\\n",
      "0                  4.27                  5.74                 0.1670   \n",
      "1                  3.75                  5.99                 0.2884   \n",
      "2                  3.59                  4.98                 0.2348   \n",
      "3                  3.29                  5.89                 0.2992   \n",
      "4                  3.69                  6.16                 0.3385   \n",
      "\n",
      "   coef_variacao_revenda  \n",
      "0               0.034125  \n",
      "1               0.064800  \n",
      "2               0.057400  \n",
      "3               0.071200  \n",
      "4               0.075500  \n",
      "Total de linhas: 1155\n",
      "Qtd regiões únicas: 5\n"
     ]
    }
   ],
   "source": [
    "fato_regiao = create_fato_precos_regiao(\n",
    "    df_silver,\n",
    "    dim_produto,\n",
    "    dim_unidade,\n",
    "    dim_tempo,\n",
    "    dim_regiao\n",
    ")\n",
    "\n",
    "print(fato_regiao.head())\n",
    "print(\"Total de linhas:\", len(fato_regiao))\n",
    "print(\"Qtd regiões únicas:\", fato_regiao[\"regiao_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "1285f5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fato fato_precos_regiao salvo em ../data/gold/2025/fato/fato_precos_regiao.csv\n"
     ]
    }
   ],
   "source": [
    "save_fato(fato_regiao, \"fato_precos_regiao\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "df5ae826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fato_precos_pais(df, dim_produto, dim_unidade, dim_tempo, dim_pais=None):\n",
    "    fato = (\n",
    "        df.merge(dim_produto[[\"produto_id\", \"produto_descricao\"]],\n",
    "                 left_on=\"produto\", right_on=\"produto_descricao\", how=\"left\")\n",
    "          .merge(dim_unidade[[\"unidade_id\", \"unidade_descricao\"]],\n",
    "                 left_on=\"unidade_medida\", right_on=\"unidade_descricao\", how=\"left\")\n",
    "          .merge(dim_tempo[[\"tempo_id\",\"data_inicial\",\"data_final\",\"ano\",\"mes_id\",\"semana\"]],\n",
    "                 on=[\"data_inicial\",\"data_final\"], how=\"left\")\n",
    "    )\n",
    "\n",
    "    \n",
    "    fato[\"pais_id\"] = 1\n",
    "\n",
    "    fato_agg = fato.groupby(\n",
    "        [\"data_inicial\",\"data_final\",\"ano\",\"mes_id\",\"semana\",\n",
    "         \"produto_id\",\"unidade_id\",\"pais_id\"],\n",
    "        as_index=False\n",
    "    ).agg({\n",
    "        \"num_postos_pesquisados\": \"sum\",\n",
    "        \"preco_medio_revenda\": \"mean\",\n",
    "        \"preco_minimo_revenda\": \"min\",\n",
    "        \"preco_maximo_revenda\": \"max\",\n",
    "        \"desvio_padrao_revenda\": \"mean\",\n",
    "        \"coef_variacao_revenda\": \"mean\"\n",
    "    })\n",
    "\n",
    "    fato_agg = fato_agg.astype({\"pais_id\": \"int64\"})\n",
    "\n",
    "    return fato_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "7ccc09e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  data_inicial  data_final   ano  mes_id  semana  produto_id  unidade_id  \\\n",
      "0   2025-01-05  2025-01-11  2025       1       1           1           1   \n",
      "1   2025-01-05  2025-01-11  2025       1       1           2           1   \n",
      "2   2025-01-05  2025-01-11  2025       1       1           3           1   \n",
      "3   2025-01-05  2025-01-11  2025       1       1           4           2   \n",
      "4   2025-01-05  2025-01-11  2025       1       1           5           3   \n",
      "\n",
      "   pais_id  num_postos_pesquisados  preco_medio_revenda  preco_minimo_revenda  \\\n",
      "0        1                   14708             4.391940                  3.29   \n",
      "1        1                   13484             6.352258                  5.44   \n",
      "2        1                   17456             6.179109                  5.04   \n",
      "3        1                    7535           108.397400                 80.00   \n",
      "4        1                    1596             4.877933                  3.79   \n",
      "\n",
      "   preco_maximo_revenda  desvio_padrao_revenda  coef_variacao_revenda  \n",
      "0                  6.16               0.173352               0.039803  \n",
      "1                  9.49               0.197071               0.031134  \n",
      "2                  7.99               0.150833               0.024634  \n",
      "3                160.00               6.658332               0.061960  \n",
      "4                  6.48               0.072541               0.015273  \n",
      "Total de linhas: 231\n",
      "Países únicos: 1\n"
     ]
    }
   ],
   "source": [
    "fato_pais = create_fato_precos_pais(\n",
    "    df_silver,\n",
    "    dim_produto,\n",
    "    dim_unidade,\n",
    "    dim_tempo\n",
    ")\n",
    "\n",
    "print(fato_pais.head())\n",
    "print(\"Total de linhas:\", len(fato_pais))\n",
    "print(\"Países únicos:\", fato_pais[\"pais_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "c2f86af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fato fato_precos_pais salvo em ../data/gold/2025/fato/fato_precos_pais.csv\n"
     ]
    }
   ],
   "source": [
    "save_fato(fato_pais, \"fato_precos_pais\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
